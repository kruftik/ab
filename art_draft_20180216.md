
# _CI/CD для бедных или DevOps-подход на  примере одного проекта на Урале_
*Действующие лица (**Команда**)*: разработчиков - 2 человека, админ - 1 человек.

> Статья повествует об использовании таких технологий, как Ansible, Docker Swarm, Jenkins и Portainer для реализации CI/CD-пайплайна с возможность контроля за ним с помощью красивого веб-интерфейса

Чего обычно хочет _разработчик_ ? Он хочет творить, не думая о деньгах, и максимально быстро видеть результаты собственного творчества.
С другой стороны _бизнес_, который хочет денег, да побольше и поэтому постоянно думает о снижении *T2M*. Другими словами, _бизнес_ мечтает об ускорении вывода *MVP* (a.k.a. Minimum Valuable Product) новых продуктов  и обновлении существующих продуктов на рынке.
Ну, а чего же хочет _админ_? А _админ_ - человек простой, он хочет играть в ~~Кваку~~ Танки и чтобы его пореже дергали _разработчики_ и _бизнес_. 
Поскольку для реализации желаний админа, как показывает правда жизни, его силами должны реализоваться и мечты других героев, представители ИТ-тусовки много работали над этим. Часто получалось достичь желаемого, придерживаясь методологии *DevOps* и реализуя принципы *CI/CD* (***C**ontinuous **I**ntegration and **D**elivery*).

Так получилось и в проекте по разработке новой версии Системы Мониторинга Банкоматов (СМБ) Банка, в которой удалось в весьма сжатые сроки реализовать полный *пайплайн* от публикации изменений исходников в системе контроля версия разработчиком до автоматического перезапуска приложения в тестовой среде.

## Часть 0. Команда
* Разработчики проекта (Dev):
	 - [@Андрей Андрей](http://alfa/ya/profile/Pages/profile.aspx?accountname=REGIONS%5CU_061KD)
	 - [@Иван Зыков](http://alfa/ya/profile/Pages/profile.aspx?accountname=REGIONS%5CU_061LU)
* Сисадмин проекта (Ops):
	- [@Дмитрий Гадеев](http://alfa/ya/profile/Pages/profile.aspx?accountname=REGIONS%5CU_061JM)

## Архитектура системы
После некоторых обсуждения, командой была выбрана следующая двухуровневая архитектура:
* Бекэнд-часть на Java, реализованный на активно-развивающемся фреймворке Spring-Boot, общающаяся с различными БД и другими корпоративными системами  (потому, что легко, быстро и понятно писать)
* Фронтэнд-часть на NodeJS (и ReactJS - интерфейс в браузере), потому что _очень-очень быстро_ работает.
* 
В свою очередь к этим компонентам был добавлен еще NGINX-сервер, являющийся фронтэндом для NodeJS-приложения. Его роль заключалась в распределении запросов между самим приложением и другими инфраструктурными компонентами системы, речь о которых пойдет ниже.

### Чего хотелось команде

Как только новому проекту был дан "зеленый свет" со стороны руководства (спасибо, [@Максим](http://alfa/) и [@Артем](http://alfa/)!), появилась первая техническая задача, а именно, подготовка "оборудования" для запуска нового проекта. Поскольку всем участникам было очевидно, что без максимальной оперативности "*выкатки*" новых версий на серверы развитие проекта будет весьма непростым, сразу же было решено идти по пути полного *CI/CD*, т.е. хотелось достичь следующего *пайплайн*-а:
* Разработчик публикует изменения ("коммит") в систему контроля версий ("гит"),
* ГИТ проводит минимально необходимое тестирование содержимого коммита на наличие необходимо атрибутики (например, правильный формат commit message), соответствие стилю оформления Банка и другой бюрократии,
* гит-сервер посредством механизма *web-hook*-ов дергает сервер непрерывной интеграции Jenkins, 
* Jenkins запускает операции скачивания актуальной версии исходников из гит-а и выполнения *CI/CD-пайплайна*:
	*  компиляция исходников и первоначальное тестирование
	* сборки новой версии Docker-образов (*неприлично же что-то деплойить в 2018-м на голое железо или виртуалку, не поймут*)
	*  публикация образов в [Artifactory](https://artapp.moscow.alfaintra.net/) (система хранения и управления артифактами, рекомендую!)
	* перезапуск новой версии приложения (или всего "стека" приложений) на сервере с "откатом" предыдущей версии в случае не очень успешного обновления.

## Рамки
У людей "в теме", наверняка, уже возник вопрос: "А чего это они какими-то костылями пользуются, а не 'production-ready'-решениями а-ля Kubernetes или Mesos / Marathon ?".  Подобный вопрос вполне разумен, особенно учитывая факт, что пара последних уже развернута в Банке и активно используется в Лаборатории.
Поэтому, сразу же скажем, что описываемое решение было использовано по целому ряду причин, в том  числе:
	* Оно проще (сильно-сильно проще) 
	* Его было легче понять всей команде и развернуть админу

Однако, мы не забываем, что выбранное нами решение относится к богатому семейству костылей и надеемся в недалеком будущем переехать на более стандартный Kubernetes, о чем конечно же вам расскажем :-)

Кроме того, эта статья касается только веб-ориентированных приложений, а также рассказывает об идеальном случае _stateless_-архитектуры, когда данные, наверняка где-то есть, но они далеко и мы об их хранении не задумываемся.

## Часть 1. Установка и базовая настройка ПО на хост-системе
С целью максимальной автоматизации и высокой воспроизводимости всей цепочки, хост-систему (виртуальная машина на базe VMWare / qemu KVM / облако / что-то еще) было решено настраивать с использованием системы управления конфигурацией _[Ansible](https://www.ansible.com/)_ от Red Hat.

Стоит добавить, что в дополнение к легкой повторяемости и воспроизводимости, использование подобных систем (кроме Ansible, существуют также системы *Puppet* и *Chef*) обладает огромным преимуществом перед использованием разнообразных shell- или python- скриптов в виде наличия *идемпотентности*, т.е. свойства, что при повторных запусках итоговое состояние системы не изменяется. 

Данное преимущество вытекает из того факта, что при использовании систем управления конфигурациями описывается не процесс достижения желаемого состояния, а само желаемое состояние в декларативной форме.

### 1.1 ssh HostKeyChecking
По-умолчанию, Ansible уважает безопасность и проверяет ssh-отпечаток удаленного конфигурируемого хоста. Т.к. в данном режиме отключена возможность авторизации по паролю, то при первоначальной настройке сервера требуется либо отключить HostKeyChecking, либо предварительно добавить отпечаток в локальный кеш. 
Последнего можно достичь целыми двумя путями:
##### или так:
Определить специальную переменную окружения:
~~~~~bash
$ export ANSIBLE_HOST_KEY_CHECKING=False
~~~~~
##### или по другому:
Добавить в локальный конфигурационный файл ansible.cfg параметр host_key_checking:
~~~~~ini
[defaults]
host_key_checking = False
~~~~~

При первом способе проверка отключается только пока существует такая переменная окружения, а при втором - полностью для этого хоста.

### 1.2 Inventory
**Inventory** - это сущность в системе Ansible, с помощью которой описываются хосты и их группы, конфигурацией которых необходимо управлять.

Inventory можно описывать в формате ini или yaml. В данном проекте был выбран последний.  
Пример hosts.yml - файла:
~~~~~yml
#_ Группа all существует всегда
all:
  hosts:
    #	имя хоста удаленной системы,  к которому будет подключаться Ansible
    some-cool-vm-host.regions.alfaintra.net
  vars:
    #	имя пользователя, кем будет авторизоваться
    ansible_user: 'root'
    # очень плохо такое делать, но тут записан пароль открытым текстом :-(
    ansible_password: '12345678'

    # Это сертификат банковского СА
    ab_ca_crt: "
 -----BEGIN CERTIFICATE-----
 MIIDjjCCAnagAwIBAgIQCYntz/Qul7NBrKAW+kYkqTANBgkqhkiG9w0BAQsFADBG
 MQswCQYDVQQGEwJSVTEWMBQGA1UEChMNSlNDIEFsZmEtQmFuazEfMB0GA1UEAxMW
 QWxmYS1CYW5rIFJvb3QgQ0EgMjAxMjAeFw0xNzA1MjMwMzQ2MDJaFw0yNzA1MjMw
 MzU2MDJaMEYxCzAJBgNVBAYTAlJVMRYwFAYDVQQKEw1KU0MgQWxmYS1CYW5rMR8w
 HQYDVQQDExZBbGZhLUJhbmsgUm9vdCBDQSAyMDEyMIIBIjANBgkqhkiG9w0BAQEF
 AAOCAQ8AMIIBCgKCAQEAoK3YwlcKAM7gdGQotZ3iEIzbEwdjm+0dSYPnxMRlocop
 pCN7esvBA7JgaLjZvdV0j6kHbjVcC3N9ase2gaXuqRTExmGoO9N/vBwo7U+1SJ0Q
 K7vqgnipjB9AUAarJr4xtrnEH8UWZGToLk5A2Lj2mKvounyTTb/juFG/YkJ4rhQ3
 qLJOOtvpIFPWw3kUyOQYg39KcR3fzqJXTFzYNZ2kF9Pi2bFAdMfiiswELhyZnfMA
 aexz4bikEhbI39FTHUTrK/wSKu6xpAml1DYA4uuFolaWvgbK2D60gHazBIWVBEN4
 aC7vX87m6ekmlLjNwTXqnelZchx/x/7nPngzl6AJVwIDAQABo3gwdjALBgNVHQ8E
 BAMCAYYwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUztTnO2tAFpxet22/Jx+r
 Uyi2aVgwEgYJKwYBBAGCNxUBBAUCAwEAATAjBgkrBgEEAYI3FQIEFgQUr5dF2u0K
 zzkBCrCNjnUi4j88xUAwDQYJKoZIhvcNAQELBQADggEBAJATy6SVDxtNoJri+BLB
 A+g8wtZRKx4PuLG9gUmmD7+M8Sw8HYiv8dxHfc8VBXLTzuh5a7X6N+71RWL67Lbv
 AFvTRT1iqdBeSKMdiB13kQUlrXrv2O/VxnMz0XonKj7sq+MfDm4A+dhB0xuAPluB
 n/I7JOdmaMY2YZmp7EmM9FsdjIAfOgvoSTPdg9JExkILAKkinry2cRIhUGKfqAHt
 QgvxzhOOX/upISkv4PwNOkVt3LAw/znPG+ppvqIJgOoYOMmK0VtEx2AGTTKgjrEK
 odnYB88D/Tm5YVrvjFrqGyhRYzrxEDtIfL/Fv66orFUOQCBGnqHe+HLk5s519Vpc
 gR0=
 -----END CERTIFICATE-----"
~~~~~
Для впервые столкнувшихся с форматом yaml хотелось бы отметить, что все отступы в данном формате необходимо оформлять **СТРОГО** с помощью **ПРОБЕЛАМИ**.

### 1.3 Playbook
**Playbook** - это еще одна сущность в Ansible, в которой непосредственно декларативно описывается желаемое конечное состояние хостов и групп из Inventory. Также как и почти все в Ansible, playbook описывается в файле (ах)  в yaml-формате.

Для того, чтобы запустить на выполнение playbook-файл, необходимо выполнить команду наподобие:
~~~~~bash
ansible-playbook -i ./hosts.yml tasks.yml
~~~~~

В данном в playbook-е описывалась полная настройка базовой системы с созданием необходимых пользователей и установкой Docker-а:
~~~~~yaml
#_ указывается маска имен хостов и групп, для которых будут выполняться нижеописанные таски (задания)
- hosts: all
  tasks:
  # Список тасков
    - name: Удаляем все репозитории из системы
      shell: rm /etc/zypp/repos.d/* || exit 0

    - name: Добавляем банковские SLES-репы на хост REPOs...
      zypper_repository: repo="{{ item.repo }}" name="{{ item.name }}" disable_gpg_check="{{ item.disable_gpg_check|default('no') }}"
      with_items:
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/ALFA/SLES12/APPS", name: "SLE-APPS" , disable_gpg_check: "yes" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Updates/SLE-SERVER/12-SP3/x86_64/update", name: "SLE-SERVER-Updates" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Products/SLE-SERVER/12-SP3/x86_64/product", name: "SLE-SERVER-Product" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Updates/SLE-Module-Web-Scripting/12/x86_64/update", name: "SLE-Module-Web-Scripting-Updates" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Products/SLE-Module-Web-Scripting/12/x86_64/product", name: "SLE-Module-Web-Scripting-Product" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Updates/SLE-Module-Containers/12/x86_64/update", name: "SLE-Module-Containers-Updates" }
        - { repo: "http://sles11-smt.moscow.alfaintra.net/repo/SUSE/Products/SLE-Module-Containers/12/x86_64/product", name: "SLE-Module-Containers-Product" }

    - name: Обновляем все пакеты в системе автоматически
      zypper: 
        name: '*'
        state: latest

    - name: Помещаем сертификат банковского CA на сервер
      copy:
        # Берем сертификат из переменных Inventory
        content: '{{ ab_ca_crt }}'
        dest: /etc/pki/trust/anchors/alfaCA_2012.crt
        owner: root
        group: root
        mode: 0644

    - name: ... и перечитываем системное хранилище
      shell: update-ca-certificates

    - name: Создаем юзеров и группы
      group: name="{{ item.name  }}" gid={{ item.gid }} state="present"
      with_items:
        - { name: "docker", gid: 1000 }
    - user: 
        name: "{{ item.name }}"
        uid: "{{ item.uid }}" 
        group: "{{ item.gid  }}"
        state: "present"
      with_items:
        - { name: "dockeradm", uid: 1000, gid: "docker" }

    - name: Меняем пароли пользователям
      user: 
        name: "{{ item }}" 
        # хеш пароля 1...8 в SHA-512
        password: "$6$PI4IbcpvgzNCWvI$urGzZAoYLiBU6HhmsjZjMyQ0VfgZbNGpzJq2CazClm5ofnhgcdf1My292Pjlh1rQhrOZK6czegfoizgE6zikT1"
        generate_ssh_key: yes
      with_items:
        - root
        - dockeradm

    - name: Добавляем публичный ssh-ключ текущего пользователя ЛОКАЛЬНОЙ машины указанным юзерам на УДАЛЕННОМ сервере
      authorized_key: 
        user: "{{ item }}" 
        key: "{{ lookup('file', '~/.ssh/id_rsa.pub') }}"
        state: "present" 
      with_items:
        - root
        - dockeradm

    - name: Создаем нужные VG в системе
      lvg:
        vg: 'vgAPP'
        pvs: '/dev/sdb'

    - name: ... и LV
      lvol: 
        vg: "{{ item.vg }}" 
        lv: "{{ item.lv }}"
        size: "{{ item.size }}"
      with_items:
        - { vg: 'vgAPP', lv: "lvData", size: "10G" }
        - { vg: 'vgAPP', lv: "lvDockerData", size: "5G" }

    - name: Создаем ФС на созданных LV-ах
      filesystem: dev="/dev/{{ item }}" fstype="btrfs"
      with_items:
        - 'vgAPP/lvData'
        - 'vgAPP/lvDockerData'

    - name: Прописываем информацию об ФС в /etc/fstab и монтируем их
      mount: path="{{ item.dst }}" src="/dev/{{ item.src }}" state="mounted" fstype="btrfs" opts="noatime"
      with_items:
        - { src: "vgAPP/lvData", dst: "/APP" }
        - { src: "vgAPP/lvDockerData", dst: "/var/lib/docker" }

    - name: Создаем всякие важные каталоги
      file: 
        path: "{{ item.path }}" 
        state: "directory" 
        # внимание не дефолтные значения
        mode: "0{{ item.perms|default('755') }}" 
        owner: "{{ item.user|default('dockeradm') }}" 
        group: "{{ item.group|default('docker') }}"
      with_items:
        - { path: '/etc/docker', user: 'root', group: 'root' }
        - { path: '/APP' }
        - { path: '/APP/configs' }
        - { path: '/APP/configs/filebeat' }
        - { path: '/APP/logs' }
        - { path: '/APP/logs/nginx' }
        - { path: '/APP/jenkins' }
        - { path: '/APP/jenkins/master' }
        - { path: '/APP/jenkins/node' }
        - { path: '/APP/portainer_data' }

    - name: Ставим нужные пакеты в систему
      zypper: 
        name: '{{ item }}'
      with_items:
        - docker
        - mc
        # Java необходима для запуска Jenkins-слейва
        - java-1_8_0-openjdk-headless
        

    # Помним о небходимости настроить подсети для Докера
    - name: Копируем конфиг-файл с "правильными сетями"
      template:
        src: daemon.json
        dest: /etc/docker/daemon.json
        owner: root
        group: root
        mode: 0644

    - name: Активируем и запускаем сервисы...
      systemd: 
        name: "{{ item }}"
        state: 'restarted'
        enabled: 'yes'
      with_items:
        - docker
        - sshd

    - name: Помещаем на целевой сервер docker-compose через скачивание на локальную
      get_url: 
        url: "https://github.com/docker/compose/releases/download/1.18.0/docker-compose-Linux-x86_64"
        dest: "/tmp/docker-compose"
      delegate_to: 127.0.0.1
  
    - copy: 
        src: "/tmp/docker-compose"
        dest: "/usr/local/bin/docker-compose"
        mode: "u=rwx,g=rx,o=rx"


    - name: Копируем конфиг-файл NGINX, конструируя его из шаблона
      template: 
        src: nginx.conf
        dest: /APP/configs/nginx.conf
        owner: dockeradm
        group: docker
        mode: '0644'

    - name: Копируем описание docker-compose - сервиса, конструируя его из шаблона
      template: 
        src: docker-compose.yml
        dest: /APP/docker-compose.yml
        owner: dockeradm
        group: docker
        mode: '0644'

    - name: Скачиваем необходимые обрацы docker
      # Костыль с while[] нужен, т.к. АртАПП ХА _ЧАСТО_ возвращает ХТТП 500 :(
      shell: while true; do docker pull {{ item }} && break; done
      with_items:
        - public-upbp.artapp/web/nginx:1.13.7
        - docker-hub.artapp/jenkins/jenkins:lts
        - docker-hub.artapp/portainer/portainer:latest

    
    - name: Запуск docker-compose - сервис с Jenkins, Portainer и NGINX перед ними
      shell: docker-compose -f /APP/docker-compose.yml up -d --force-recreate

    - name: Ожидаем окончания инициализации Jenkins-а
      wait_for:
        path: '/APP/jenkins/master/secrets/initialAdminPassword'

    - name: Получаем значение временного пароля Jenkins
      fetch: 
        src: '/APP/jenkins/master/secrets/initialAdminPassword'
        dest: initialJenkinsAdminPassword.txt
        flat: yes
~~~~~

## Часть 2. Сервисы проекта
### 2.1 CI-сервер и процесс
За процесс "непрерывной интеграции и деплоймента" в проекте отвечает хорошо многим известный сервер [Jenkins CI](https://jenkins.io/). 

Код Ansible playbook-и выше устроен так, что в конце его выполнения на сервере уже запущен свежеустановленный Jenkins (в Docker-контейнере), а его временный пароль сохранен на _ЛОКАЛЬНОЙ_ машине в файле **initialJenkinsAdminPassword.txt**.

Поскольку всей команде хотелось максимально приблизиться к идеальному случаю [Infrastructure as code](https://en.wikipedia.org/wiki/Infrastructure_as_Code) (IaC), то в проекте "таски" были реализованы в виде [декларативных и скриптованных пайплайнов](https://jenkins.io/doc/book/pipeline/syntax/) Jenkins-а, когда задачи описываются на языке Groovy Script, а сам их код хранится рядом с исходниками проекте в системе контроля версий (git).

Пример пайплайна сборки backend-части приложения на Spring Boot показан ниже:
~~~~~groovyscript
pipeline {
    agent {
        # указываем, что выполнять задачу хотим внутри 
        # Docker-контейнера на базе указанного образа:
        docker {
            image 'java:8-jdk'
        }
    }
    
    stages {
        stage('Стягиваем код из ГИТа') {
            steps {
                checkout scm
            }
        }
        stage('Собираем') {
            steps {
                sh 'chmod +x ./gradlew'
                sh './gradlew build -x test'
            }

        }
        stage('Тестируем') {
            steps {
                script {
                    sh './gradlew test'
                }
            }
        }
    }
}

#_ Этап сборки нового Docker-образа и его загрузки с систему Artifactory:
node {
    stage('Собираем образ') {
        docker.withRegistry("https://repo.artapp", "LoginToArtifactory") {
            def dkrImg = docker.build("smb-ng.artapp/dev-backend:${env.BUILD_ID}")
            dkrImg.push()
            dkrImg.push('latest')
        }
	}
    stage('Заливаем его в Artifactory') {
        docker.withRegistry("https://repo.artapp", "LoginToArtifactory") {
            sh "docker service update --image smb-ng.artapp/dev-backend:${env.BUILD_ID} SMB_dev-backend"
        }
    }
}
~~~~~
Отдельно хотелось бы отметить, что при сборке образов, каждая версия получает свой тег (метку), что ***значительно*** облегчает процесс авторестарта приложения.

### 2.2 Portainer
Для облегчения взаимодействия всех членов команды с Docker, в проекте нами был использован простой веб-интерфейс для него - [Portainer](https://portainer.io/). Данное приложение, также как и сам Docker, написано на языке Go, а поэтому отличается высокой производительностью при чрезвычайно легком развертывании. 

Например, в простейшем случае следующая команда приведет к запуску Портейнера на порту 9000 хост-системы:
~~~~~bash
docker run -d \
	-p 9000:9000 \
	-v /var/run/docker.sock:/var/run/docker.sock \
portainer/portainer
~~~~~

Однако, в текущем проекте было решено воспользоваться функционалом средства "оркестрации" для одного хоста - [Docker Compose](https://docs.docker.com/compose/).

### 2.3 Docker - контейнеры и сервисы
Все необходимые приложения и сервисы в данном проекте запускаются посредством простого docker-compose.yml-файла.

Базовый набор "инфраструктурных" сервисов запускается посредством следующего описания:

~~~~~yaml
version: '3.2'

services:
  # Непосредственно общающийся  с пользователями NGINX
  nginx:
    image: "public-upbp.artapp/web/nginx:1.13.7"
    container_name: fe-nginx
    restart: always
    volumes:
      - /APP/configs/nginx.conf:/etc/nginx/nginx.conf
      - /APP/logs/nginx:/var/log/nginx
      - /usr/share/zoneinfo/Europe/Moscow:/etc/localtime:ro
    networks:
      - int
    ports:
      - "80:80/tcp"
      - "8080:80/tcp"

  # Jenkins CI - сервер, который отвечает за CI/CD-процесс
  ci:
    image: "docker-hub.artapp/jenkins/jenkins:lts"
    container_name: ci-jenkins
    restart: always
    volumes:
        - /usr/share/zoneinfo/Europe/Moscow:/etc/localtime:ro
        - /APP/jenkins/master:/var/jenkins_home
    environment:
      JENKINS_OPTS: '--prefix=/jenkinsci'
      JAVA_OPTS: '-Xmx512m'
    networks:
      int:
        aliases:
          - srv-ci

  # Веб-интерфейс для Docker-а
  portainer:
    image: "docker-hub.artapp/portainer/portainer:latest"
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
      - type: bind
        source: /APP/portainer_data
        target: /data
    networks:
      int:
        aliases:
          - srv-portainer
    command: -H 'unix:///var/run/docker.sock'

networks:
  int:
    external: true
~~~~~

### 2.4 Docker Swarm - кластер без кластера
Как можно видеть в файле docker-compose.yml выше, во-первых, отсутствуют упоминания бэкенд и фронтенд- частей приложения, а также, присутствует ссылка на "внешнюю" (*external: true*) сеть по имени int. Внешними являются любые ресурсы (сети, тома и другие существующие сущности), не объявленные в одном файле.

Дело в том, что в проекте нам требовалась иметь возможность проводить рестарт "сервисов" при обновлении версии образа в Docker-репозитории Artifactory, а подобный функционал присутствует в сервисах Docker Swarm (встроенная в Docker multi-master система оркестрации Docker-контейнеров) что называется "из коробки". Данная возможность реализуется через возможность изменить требуемый образ у запущенного сервиса и при наличии новой версии образа в репозитории, перезапуск произойдет автоматически. В случае же, если версия не изменилась - сервисный контейнер продолжает штатно выполняться.

Что касается сети, запуская приложение в виде Docker Swarm - сервиса (yaml-описание представлено ниже), нам требовалось сохранить сетевую связность его компонентов и сервера NGINX, объявленного выше. Это было достигнуто через создание на сервере кластерой Overlay - сети, в которую включались и базовые сервисы, описанные выше, и непосредственно компоненты приложения:

~~~~~bash
docker network create -d overlay --subnet 10.129.106.254/24 --attachable int
~~~~~
(ключ --attachable является необходимым, т.к. без него базовые сервисы не имеют доступа к кластерной сети)

Описание компонентов приложения c 2-мя сервисами:
~~~~~yaml
version: '3.2'

services:
  pre-live-backend:
    image: repo.artapp/pre-live-backend:latest
    deploy:
      mode: replicated
      replicas: 1
    networks:
      - int
  pre-live-front:
    image: repo.artapp/pre-live-front:latest
    deploy:
      mode: replicated
      replicas: 1
    depends_on:
      - pre-live-backend
    environment:
     - PROXY=http://pre-live-backend:8080
    networks:
      - int
networks:
  int:
    external: true
~~~~~

## Заключение

Как было отмечено в начале, на старте проекта команде хотелось получить все преимущества DevOps-подхода, в частности организовать процесс непрерывной доставки кода от гит-репозитория до "боевого" сервера в виде запущенного на нем приложения. При этом на текущем этапе не хотелось полностью уходить от наработанных практик и полностью перестраивать себя под жизнь в мире больших оркестраторов. 
Описанная архитектура системы, которая была продумана и реализована менее, чем за 2 недели рабочего времени (совместно с другими проектами, на которыми трудились члены команды) в итоге позволила нам достичь желаемого.
Мы полагаем, что данный материал должен быть интересным и полезным и другим другим командам в Банке.
